{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from data_loader_ddr_fixed_length import get_loader \n",
    "from build_vocab_ddr import Vocabulary\n",
    "from model_fixed_length import EncoderCNN, DecoderRNN\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "import nltk\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Image Captioning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    model_path = 'ddrmodels/'\n",
    "    crop_size = 224\n",
    "    vocab_path = 'data/vocab_ddr.pkl'\n",
    "    image_dir = 'data/resizedddr'\n",
    "    caption_path = 'data/annotations/spectrogram_2.csv'\n",
    "    log_step = 10\n",
    "    save_step = 100\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    num_layers = 3\n",
    "    num_epochs = 1\n",
    "    batch_size = 2\n",
    "    num_workers = 2\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1000]\n",
      "torch.Size([2, 3, 224, 224])\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 1000, 256])\n",
      "torch.Size([2, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at c:\\a\\w\\1\\s\\tmp_conda_3.7_061434\\conda\\conda-bld\\pytorch_1544163540495\\work\\aten\\src\\thc\\generic/THCTensorMath.cu:74",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5e948337cc84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-5e948337cc84>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\pytorch-tutorial\\tutorials\\03-advanced\\image_captioning\\model_fixed_length.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, features, captions, lengths)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mpacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mhiddens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at c:\\a\\w\\1\\s\\tmp_conda_3.7_061434\\conda\\conda-bld\\pytorch_1544163540495\\work\\aten\\src\\thc\\generic/THCTensorMath.cu:74"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    CAPTION_LENGTH = 1000\n",
    "    \n",
    "    \n",
    "    # Create model directory\n",
    "    if not os.path.exists(args.model_path):\n",
    "        os.makedirs(args.model_path)\n",
    "    \n",
    "    # Image preprocessing, normalization for the pretrained resnet\n",
    "    transform = transforms.Compose([ \n",
    "        transforms.RandomCrop(args.crop_size),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load vocabulary wrapper\n",
    "    with open(args.vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    # Build data loader\n",
    "    data_loader = get_loader(args.image_dir, args.caption_path, vocab, \n",
    "                             transform, args.batch_size,\n",
    "                             shuffle=True, num_workers=args.num_workers) \n",
    "\n",
    "    # Discriminator\n",
    "    d_encoder = EncoderCNN(args.embed_size).to(device)\n",
    "    d_decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers).to(device)\n",
    "    \n",
    "    D = nn.Sequential(\n",
    "        nn.Linear(CAPTION_LENGTH, 256),\n",
    "        #nn.Linear(len(vocab), 256),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(256, 1),\n",
    "        nn.Sigmoid())\n",
    "       \n",
    "    D = D.to(device)\n",
    "\n",
    "    # Generator\n",
    "    \n",
    "    g_encoder = EncoderCNN(args.embed_size).to(device)\n",
    "    g_decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers).to(device)\n",
    "\n",
    "    # Binary cross entropy loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    d_params = list(D.parameters()) + list(d_encoder.parameters()) + list(d_decoder.parameters())\n",
    "    d_optimizer = torch.optim.Adam(d_params, lr=0.0002)\n",
    "    \n",
    "    g_params = list(g_decoder.parameters()) + list(g_encoder.parameters())\n",
    "    g_optimizer = torch.optim.Adam(g_params, lr=0.0002)\n",
    "\n",
    "    def denorm(x):\n",
    "        out = (x + 1) / 2\n",
    "        return out.clamp(0, 1)\n",
    "\n",
    "    def reset_grad():\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "    # Start training\n",
    "    total_step = len(data_loader)\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "            \n",
    "            # Set mini-batch dataset\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "            # Create the labels which are later used as input for the BCE loss\n",
    "            real_labels = torch.ones(args.batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(args.batch_size, 1).to(device)\n",
    "\n",
    "            # ================================================================== #\n",
    "            #                      Train the discriminator                       #\n",
    "            # ================================================================== #\n",
    "\n",
    "            # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "            # Second term of the loss is always zero since real_labels == 1\n",
    "            #outputs = D(images)\n",
    "            #outputs = decoder(features, captions, lengths)\n",
    "            print(lengths)\n",
    "            print(images.shape)\n",
    "            features = d_encoder(images)\n",
    "            print(features.shape)\n",
    "            outputs = d_decoder(features, captions, lengths)\n",
    "            print(outputs.shape)\n",
    "            print(captions.shape)\n",
    "            \n",
    "            d_outputs = D(outputs)\n",
    "            print(\"d_outputs: \", d_outputs.shape, \"Real Labels: \", real_labels.shape)\n",
    "            d_loss_real = criterion(d_outputs, real_labels)\n",
    "            real_score = outputs\n",
    "\n",
    "            # Compute BCELoss using fake images\n",
    "            # First term of the loss is always zero since fake_labels == 0\n",
    "            z = torch.randn(args.batch_size, latent_size).to(device)\n",
    "            fake_images = G(z)\n",
    "            outputs = D(fake_images)\n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "            fake_score = outputs\n",
    "\n",
    "            # Backprop and optimize\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            reset_grad()\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # ================================================================== #\n",
    "            #                        Train the generator                         #\n",
    "            # ================================================================== #\n",
    "\n",
    "            # Compute loss with fake images\n",
    "            z = torch.randn(args.batch_size, latent_size).to(device)\n",
    "            fake_images = G(z)\n",
    "            outputs = D(fake_images)\n",
    "\n",
    "            # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "            # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "\n",
    "            # Backprop and optimize\n",
    "            reset_grad()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            if (i+1) % 200 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "                      .format(epoch, args.num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n",
    "                              real_score.mean().item(), fake_score.mean().item()))\n",
    "\n",
    "        # Save real images\n",
    "        if (epoch+1) == 1:\n",
    "            images = images.reshape(images.size(0), 1, 28, 28)\n",
    "            save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
    "\n",
    "        # Save sampled images\n",
    "        fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "        save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
    "\n",
    "    # Save the model checkpoints \n",
    "    torch.save(G.state_dict(), 'G.ckpt')\n",
    "    torch.save(D.state_dict(), 'D.ckpt')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
